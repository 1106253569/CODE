# Spark复习

| 符号 |       含义       |
| :--: | :--------------: |
| ％d |    十进制数字    |
| ％s |      字符串      |
| ％c |       字符       |
| ％e |    指数浮点数    |
| ％f |      浮点数      |
| ％i |  整数（十进制）  |
| ％o |      八进制      |
| ％u |   无符号十进制   |
| ％x |     十六进制     |
| s" " |  一个模板字符串  |
| f" " | 格式化字符串模板 |

Scala中打印“%”，可以使用两个百分号来转义，如：`println("打印%%")`

## 重点

### Scala语言的概念、特点；变量常量的声明方式；基本的数据类型有哪些？

1. Scala语言的概念和特点：

   1. Scala（全称：Scalable Language）是一种静态类型的编程语言，旨在融合面向对象编程和函数式编程的特性。
   2. 它运行在Java虚拟机（JVM）上，可以与现有的Java代码进行互操作，并且具有强大的表达能力和灵活性。
   3. Scala具有简洁的语法和丰富的特性，使得编写可维护、可扩展的代码变得更加容易。
2. 变量和常量的声明方式：

   1. 变量（Variable）是可以改变其值的存储区域，而常量（Value）是不可变的。
   2. 变量可以使用关键字var进行声明：`var variableName: DataType = initialValue`
   3. 常量可以使用关键字val进行声明：`val constantName: DataType = initialValue`
3. Scala提供了丰富的数据类型，包括：

   * 整数类型：Byte、Short、Int、Long
   * 浮点数类型：Float、Double
   * 字符类型：Char
   * 布尔类型：Boolean
   * 字符串类型：String
   * 空类型：Null
   * 无类型：Unit

### Scala列表的创建方式；不可变数组的创建方式；数组中元素的访问；数组的三种循环方式能会写代码；

1. Scala中的列表（List）是不可变的，可以通过以下方式进行创建：

   1. 使用List关键字和元素值创建列表：`val list = List(1, 2, 3, 4, 5)`
   2. 使用Nil关键字创建空列表：`val emptyList = Nil`
   3. 使用::操作符（也可以使用cons方法）添加元素到列表：`val newList = 1 :: 2 :: 3 :: Nil`
2. 不可变数组的创建方式：

   1. Scala中的不可变数组可以使用Array关键字进行创建，数组长度不可变，但数组中的元素可以修改。
   2. 创建不可变数组的方式：
      * 使用Array关键字和元素值创建数组：`val array = Array(1, 2, 3, 4, 5)`
      * 使用new Array关键字和数组长度创建数组：`val newArray = new ArrayInt(5)`
3. 数组中元素的访问：

   * 可以使用索引访问数组中的元素，索引从0开始。
   * 例如，访问数组array中的第一个元素：`val firstElement = array(0)`
4. 数组的三种循环方式：

   1. 使用for循环遍历数组的元素：

      ```
      for (element <- array) {
      println(element)
      }
      ```
   2. 使用索引和until函数遍历数组的元素：

      ```
      for (i <- 0 until array.length) {
      println(array(i))
      }
      ```
   3. 使用foreach方法遍历数组的元素：

      ```
      array.foreach { element =>
      println(element)
      }
      ```

### 元组的创建及元组中元素的访问

1. 元组（Tuple）是Scala中的一种数据结构，可以将多个元素组合在一起形成一个不可变的集合。元组可以包含不同类型的元素，并且长度固定。
2. 以下是元组的创建和元素访问的示例：
   1. 创建元组：
      1. 使用圆括号和逗号将元素括起来创建元组：`val tuple = (1, "hello", 3.14)`
      2. 可以省略括号，直接写入元素创建元组：`val tuple = 1 -> "hello" -> 3.14`
   2. 元素的访问：
      1. 使用索引（从1开始）来访问元组中的元素：`val firstElement = tuple._1`
      2. 可以使用._2、._3等方式访问其他位置的元素：`val secondElement = tuple._2`
      3. 可以使用模式匹配来获取元组中的各个元素：
         ```
         val (first, second, third) = tuple
         println(first)  // 输出: 1
         println(second)  // 输出: hello
         println(third)  // 输出: 3.14
         ```
      4. 需要注意的是，元组中的元素访问使用下划线 _ 后跟数字索引的形式，索引从1开始。另外，元组是不可变的，无法修改其中的元素。

### Scala辅助构造器的描述及特点；Scala伴生类和伴生对象的概念、产生条件及访问控制；

1. Scala辅助构造器：

   1. 辅助构造器是在类中定义的额外构造器，用于创建对象时提供不同的参数组合。
   2. Scala中的辅助构造器通过关键字def和类名来定义，可以有多个辅助构造器。
   3. 辅助构造器的命名为this，可以接受不同的参数列表。
   4. 辅助构造器内部必须调用主构造器或其他辅助构造器，以确保对象的正确初始化。
2. Scala伴生类和伴生对象：

   1. 伴生类（Companion Class）是与其同名的伴生对象（Companion Object）所关联的类
   2. 伴生类和伴生对象必须在同一个源文件中定义。
   3. 伴生类和伴生对象之间可以互相访问彼此的私有成员。
   4. 伴生对象可以访问伴生类的私有构造器，而在Scala中，类的构造器可以是私有的。
   5. 伴生对象可以作为工厂对象，提供创建伴生类对象的方法，类似于Java中的静态工厂方法。
   6. 伴生对象中的成员可以直接访问，而不需要通过创建对象来访问。
3. 产生条件：

   * 伴生类和伴生对象之间的关系是通过类名相同，且定义在同一个源文件中来建立的。
   * 伴生类和伴生对象的名称必须一致。
4. 访问控制：

   * 伴生类和伴生对象可以相互访问对方的私有成员，因为它们共享了一个名称空间。
   * 对于伴生类中的私有成员，伴生对象可以直接访问。
   * 对于伴生对象中的私有成员，伴生类也可以直接访问。
   * 对于其他类，无法直接访问伴生类或伴生对象中的私有成员，因为它们的访问权限仅限于伴生类和伴生对象之间。

### Scala方法的可变参数特点及定义方式；

> Scala中的可变参数（Variable Arguments）允许在方法的参数列表中接受可变数量的参数。可变参数具有以下特点：

1. 可变参数定义方式：
   * 可变参数使用*修饰符来标记，放置在方法的参数类型之前。
   * 可变参数必须是方法的最后一个参数。
2. 可变参数的特点：
   * 可变参数允许传递任意数量的参数，包括零个参数。
   * 可变参数在方法内部被当作一个数组（Array）处理，可以使用数组的相关操作进行处理。
   * 调用方法时，可以直接传递多个参数，也可以传递一个数组（或序列）作为参数。
   * 下面是可变参数的定义方式的示例：

     ```
     def printNames(names: String*): Unit = {
     for (name <- names) {
     	println(name)
         }
     }
     // 调用可变参数的方法
     printNames("Alice", "Bob", "Charlie")  // 直接传递多个参数
     printNames(Array("Alice", "Bob", "Charlie"): _)  // 传递数组作为参数
     ```

     > *在上述示例中，`printNames`方法定义了一个可变参数 `names`，类型为 `String`*。在方法体内部，`names`被当作一个数组处理，使用了 `for`循环遍历输出每个名称。调用方法时可以直接传递多个参数，也可以传递一个数组（通过 `: _*`转换为可变参数）作为参数。
     >

### Spark的概念、特点、优势（与Hadoop对比）；Spark的集群管理框架、部署模式；

> Spark是一种快速、通用、可扩展的分布式计算系统，具有以下概念、特点和优势：

1. 概念：
   1. Resilient Distributed Datasets (RDDs)：Spark的核心数据抽象，是不可变的分布式对象集合，可以在集群上并行操作。
   2. 数据流（Dataflow）：Spark通过数据流的方式进行计算，可以通过多个转换操作构建数据处理流水线。
   3. 转换操作（Transformations）和动作操作（Actions）：Spark提供了丰富的转换和动作操作，可以对RDD进行转换和操作。
   4. 延迟计算（Lazy Evaluation）：Spark采用延迟计算的方式，只有在遇到动作操作时才会触发计算。
   5. 内存计算（In-Memory Computing）：Spark将数据存储在内存中，以提高计算性能。
2. 特点：
   1. 快速性能：Spark具有高速的内存计算能力和优化的执行引擎，可以加速数据处理任务的执行。
   2. 简单易用：Spark提供了简洁的API，支持多种编程语言（如Scala、Java、Python）进行开发，并提供了丰富的库和工具。
   3. 可扩展性：Spark可以轻松地扩展到大规模的数据集和集群，利用分布式计算能力进行并行处理。
   4. 多种数据处理模式：Spark支持批处理、交互式查询和流式处理等多种数据处理模式，适用于不同的应用场景。
3. 优势（与Hadoop对比）：
   1. 更快的计算速度：相比于基于磁盘的计算框架（如Hadoop MapReduce），Spark利用内存计算和数据缓存等技术，可以显著提高计算速度。
   2. 更灵活的数据处理：Spark提供了丰富的转换和动作操作，可以进行复杂的数据处理和分析，支持SQL查询、图计算、机器学习等多种数据处理任务。
   3. 更强大的内存管理：Spark能够更有效地管理内存资源，包括内存数据存储、数据分片、数据共享等，提供更高效的内存计算能力。
   4. 更广泛的生态系统：Spark具有庞大的生态系统，包括Spark SQL、Spark Streaming、MLlib（机器学习库）和GraphX（图计算库）等，支持多种数据处理和分析需求。
4. Spark的集群管理框架和部署模式：
   1. Spark可以在多种集群管理框架上运行，包括Standalone、Hadoop YARN、Apache Mesos等。
   2. 在Standalone模式下，Spark自带了一个简单的集群管理器，可以方便地在独立的Spark集群上部署和运行。
   3. 在Hadoop YARN模式下，Spark可以利用YARN作为资源管理器，与其他Hadoop生态系统工具（如HDFS）无缝集成。
   4. 在Mesos模式下，Spark可以与Mesos集成，共享Mesos集群资源进行分布式计算。

### RDD的特点及五大特性，RDD算子的分类及区别，常用的操作类算子和行动类算子；

1. RDD（Resilient Distributed Datasets）是Spark的核心数据抽象，它具有以下特点：
   * 弹性：RDD具有容错性，可以在节点故障时自动恢复数据，保证计算的可靠性。
   * 分区：RDD将数据分割为多个分区，每个分区可以在不同的节点上并行处理，实现数据的并行计算。
   * 可变性：RDD支持两种类型的操作，即转换操作和行动操作。转换操作生成新的RDD，而行动操作将RDD的结果返回给驱动程序或写入外部存储。
   * 惰性计算：RDD采用惰性计算方式，只有在遇到行动操作时才会触发计算，可以优化计算过程。
   * 缓存：RDD支持将数据缓存在内存中，以便在后续的计算中复用，提高计算性能。
2. RDD算子可分为两类：转换操作和行动操作。

### Spark的核心模块及每个模块的功能；

1. Spark Core：
   1. Spark Core是Spark的基础模块，提供了Spark的基本功能和API。
   2. 包括任务调度、内存管理、错误恢复、分布式存储等核心功能，是其他模块的基础。
2. Spark SQL：
   1. Spark SQL是Spark用于处理结构化数据的模块，提供了用于查询结构化数据的API和SQL查询语言。
   2. 它支持将结构化数据（如JSON、Parquet、Hive表等）加载到RDD中，并提供了DataFrame和DataSet这两个高级抽象，使得可以在结构化数据上进行高性能的数据处理和分析。
3. Spark Streaming：
   1. Spark Streaming是Spark用于处理实时流数据的模块，它支持将实时数据流划分为小的批次进行处理。
   2. 它提供了类似于Spark Core的API，可以在流数据上执行转换操作和行动操作，实现实时数据的处理和分析。
4. MLlib：
   1. MLlib是Spark的机器学习库，提供了丰富的机器学习算法和工具，使得在大规模数据上进行机器学习变得更加容易。
   2. MLlib支持常见的机器学习任务，如分类、回归、聚类、推荐等，并提供了分布式的算法实现。
5. GraphX：
   1. GraphX是Spark的图计算库，提供了用于图计算的API和算法。
   2. 它将图表示为分布式的属性图，并提供了一组用于处理图结构的操作，如图的构建、遍历、连通性分析、图算法等。

### RDD的创建方式有几种？每种方式如何创建，可以进行代码举例说明；

在Spark中，可以使用以下几种方式创建RDD：

1. 从已存在的集合或数据源创建RDD：
   * 可以通过并行化已有的集合或读取外部数据源来创建RDD。
     ```
     // 从集合创建RDD
     val rdd1 = sparkContext.parallelize(Seq(1, 2, 3, 4, 5))
     // 从文件读取创建RDD
     val rdd2 = sparkContext.textFile("path/to/file.txt")
     ```
2. 通过转换已有的RDD创建新的RDD：
   * 可以对现有的RDD应用转换操作来创建新的RDD。
     ```
     val rdd1 = sparkContext.parallelize(Seq(1, 2, 3, 4, 5))
     // 使用map转换操作创建新的RDD
     val rdd2 = rdd1.map(_ * 2)
     // 使用filter转换操作创建新的RDD
     val rdd3 = rdd1.filter(_ > 3)
     ```
3. 通过外部存储系统（如HDFS）创建RDD：
   * 可以通过读取外部存储系统中的数据来创建RDD。
     ```
     val rdd = sparkContext.textFile("hdfs://path/to/file.txt")
     ```
4. 通过并行化已有的集合创建PairRDD（键值对RDD）：
   * 可以将已有的集合转换为键值对形式的RDD。
     ```
     val pairs = Seq(("a", 1), ("b", 2), ("c", 3))
     val pairRDD = sparkContext.parallelize(pairs)
     ```
5. 通过外部数据源创建PairRDD（键值对RDD）：
   1. 可以从外部数据源中读取数据，并将其转换为键值对形式的RDD。
      ```
      val pairRDD = sparkContext.textFile("path/to/file.txt").map(line => {
      	val Array(key, value) = line.split(",")
      	(key, value.toInt)
      })
      ```

### 宽依赖于窄依赖的区别，Stage划分的依据；

1. 区别
   * 窄依赖
     * 指每个父RDD的一个Partition最多被子RDD的一个Partition所使用
     * 父RDD的一个分区对应子RDD的一个分区（一对一的关系）
     * 父RDD的多个分区对应了子RDD的一个分区（多对一的关系）
   * 宽依赖
     * 指一个父RDD的一个Partition会被子RDD的多个Partition所使用
     * 父RDD的一个分区对应了子RDD的多个分区（一对多）
     * 父RDD的多个分区对应子RDD的多个分区（多对多）
2. Stage划分
   1. 当存在宽依赖时，Spark将生成一个新的阶段，将具有宽依赖的转换操作划分到同一个阶段中。
   2. 通过将转换操作划分为多个阶段，Spark可以实现更好的并行化和任务调度，提高整体作业的性能和效率。
   3. RDD是一个大的数据集合，该集合被划分成多个子集合分布到了不同的节点上，而每个子集合就称为分区（Partition）。

### RDD缓存的方法，缓存的存储级别有哪些？默认的是哪个？

### 累加器的概念、特点

### Spark SQL的概念；SparkSQL编程的入口；Spark RDD编程的入口；SparkSQL的两大常用数据集合；

### SparkRDD应用程序（读取本地或分布式文件系统中的文件，对文件内容进行特殊处理，并完成统计并输出到本地文件系统或分布式文件系统中）
