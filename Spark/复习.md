# Spark复习

## 重点

### Scala语言的概念、特点；变量常量的声明方式；基本的数据类型有哪些？

1. Scala语言的概念和特点：

   1. Scala（全称：Scalable Language）是一种静态类型的编程语言，旨在融合面向对象编程和函数式编程的特性。
   2. 它运行在Java虚拟机（JVM）上，可以与现有的Java代码进行互操作，并且具有强大的表达能力和灵活性。
   3. Scala具有简洁的语法和丰富的特性，使得编写可维护、可扩展的代码变得更加容易。
2. 变量和常量的声明方式：

   1. 变量（Variable）是可以改变其值的存储区域，而常量（Value）是不可变的。
   2. 变量可以使用关键字var进行声明：`var variableName: DataType = initialValue`
   3. 常量可以使用关键字val进行声明：`val constantName: DataType = initialValue`
3. Scala提供了丰富的数据类型，包括：

   * 整数类型：`Byte`、`Short`、`Int`、`Long`
   * 浮点数类型：`Float`、`Double`
   * 字符类型：`Char`
   * 布尔类型：`Boolean`
   * 字符串类型：`String`
   * 空类型：`Null`
   * 无类型：`Unit`

### Scala列表的创建方式；不可变数组的创建方式；数组中元素的访问；数组的三种循环方式能会写代码；

1. Scala列表（List）的创建方式：

   1. 使用 `List`关键字创建不可变列表（Immutable List）：`val list: List[Int] = List(1, 2, 3, 4, 5)`
   2. 使用 `Nil`创建空列表：`val emptyList: List[String] = Nil`
   3. 使用 `::`操作符在列表头部添加元素：`val listWithElement: List[Int] = 1 :: 2 :: 3 :: Nil`
   4. 使用 `List.tabulate`方法创建指定长度的列表：`val generatedList: List[Int] = List.tabulate(5)(_ * 2) // 生成列表：List(0, 2, 4, 6, 8)`
2. Scala可变列表（ListBuffer）的创建方式：

   1. 首先导入 `scala.collection.mutable.ListBuffer`：
   2. 创建空的可变列表：`val listBuffer: ListBuffer[String] = ListBuffer.empty[String]`
   3. 使用 `+=`操作符在列表末尾添加元素：
      ```
      listBuffer += "apple"
      listBuffer += "banana"
      listBuffer += "orange"
      ```
3. 不可变数组（Array）的创建方式：

   1. 使用 `Array`关键字创建不可变数组：`val array: Array[Int] = Array(1, 2, 3, 4, 5)`
   2. 使用 `Array.ofDim`方法创建指定长度的数组：`val emptyArray: Array[String] = Array.ofDim[String](3)`
   3. 使用数组字面量创建数组：`val arrayLiteral: Array[Int] = Array(1, 2, 3, 4, 5)`
4. 数组中元素的访问：

   1. 使用索引访问数组元素，索引从0开始：`val element: Int = array(2) // 访问索引为2的元素`
5. 数组的三种循环方式：

- 使用 `for`循环遍历数组：

  ```scala
  for (element <- array) {
  println(element)
  }
  ```
- 使用 `foreach`方法遍历数组：

  ```scala
  array.foreach(element => println(element))
  ```
- 使用 `while`循环和索引遍历数组：

  ```scala
  var i = 0
  while (i < array.length) {
    println(array(i))
    i += 1
  }
  ```

> 注意：列表（List）是不可变的，而列表缓冲（ListBuffer）是可变的。数组（Array）既可以是不可变的，也可以是可变的，取决于使用的是 `Array`还是 `ArrayBuffer`。以上代码示例均为不可变列表（List）和不可变数组（Array）的创建和操作方式。

### 元组的创建及元组中元素的访问

1. 元组（Tuple）是Scala中的一种数据结构，可以将多个元素组合在一起形成一个不可变的集合。元组可以包含不同类型的元素，并且长度固定。
2. 以下是元组的创建和元素访问的示例：
   1. 创建元组：
      1. 使用圆括号和逗号将元素括起来创建元组：`val tuple = (1, "hello", 3.14)`
      2. 可以省略括号，直接写入元素创建元组：`val tuple = 1 -> "hello" -> 3.14`
   2. 元素的访问：
      1. 使用索引（从1开始）来访问元组中的元素：`val firstElement = tuple._1`
      2. 可以使用._2、._3等方式访问其他位置的元素：`val secondElement = tuple._2`
      3. 可以使用模式匹配来获取元组中的各个元素：
         ```scala
         val (first, second, third) = tuple
         println(first)  // 输出: 1
         println(second)  // 输出: hello
         println(third)  // 输出: 3.14
         ```
      4. 需要注意的是，元组中的元素访问使用下划线 _ 后跟数字索引的形式，索引从1开始。另外，元组是不可变的，无法修改其中的元素。

### Scala辅助构造器的描述及特点；Scala伴生类和伴生对象的概念、产生条件及访问控制；

1. Scala辅助构造器：

   1. 辅助构造器是在类中定义的额外构造器，用于方便地创建对象时提供不同的参数选项。
   2. 辅助构造器使用 `def this(...)`的语法声明，在类内部定义多个辅助构造器时，它们之间通过调用主构造器或其他辅助构造器来相互关联。
   3. 辅助构造器必须以调用主构造器或其他辅助构造器的方式开始。
   4. 特点：
      * 辅助构造器的名称为 `this`，没有返回类型。
      * 辅助构造器不能直接被外部代码调用，只能在创建对象时由其他构造器调用。
2. Scala伴生类和伴生对象：

   1. 伴生类（Companion Class）是与伴生对象（Companion Object）同名且在同一个文件中定义的类。
   2. 伴生类和伴生对象之间可以互相访问彼此的私有成员。
   3. 伴生类和伴生对象必须在同一个源文件中定义，且类名和对象名相同。
   4. 产生条件：
      * 伴生类和伴生对象必须在同一个源文件中定义。
      * 类名和对象名必须相同。
   5. 访问控制：
      * 伴生类和伴生对象可以相互访问彼此的私有成员，因为它们共享同一个类作用域。
      * 伴生对象中的成员可以通过伴生类的实例访问，反之亦然。
      * 伴生对象的成员可以被其他对象直接访问，无需通过创建类的实例。
3. 示例：

```scala
class MyClass(private val name: String) {
  def display(): Unit = {
    println(s"Name: $name")
  }
}

object MyClass {
  def createInstance(): MyClass = {
    new MyClass("John")
  }
}

// 使用伴生对象的成员
val instance = MyClass.createInstance()
instance.display()
```

在上述示例中，`MyClass`是伴生类，`MyClass`是伴生对象。伴生对象的 `createInstance`方法可以直接创建 `MyClass`的实例，并且通过实例的 `display`方法访问伴生类的私有成员 `name`。

### Scala方法的可变参数特点及定义方式；

> Scala中的可变参数（Variable Arguments）允许在方法的参数列表中接受可变数量的参数。

1. 可变参数定义方式：
   * 可变参数使用 `*`修饰符来标记，放置在方法的参数类型之前。
   * 可变参数必须是方法的最后一个参数。
2. 可变参数的特点：
   * 可变参数允许传递任意数量的参数，包括零个参数。
   * 可变参数在方法内部被当作一个数组（Array）处理，可以使用数组的相关操作进行处理。
   * 调用方法时，可以直接传递多个参数，也可以传递一个数组（或序列）作为参数。
   * 下面是可变参数的定义方式的示例：

     ```scala
     def printNames(names: String*): Unit = {
     for (name <- names) {
     	println(name)
         }
     }
     // 调用可变参数的方法
     printNames("Alice", "Bob", "Charlie")  // 直接传递多个参数
     printNames(Array("Alice", "Bob", "Charlie"): _)  // 传递数组作为参数
     ```

     > *在上述示例中，`printNames`方法定义了一个可变参数 `names`，类型为 `String`*。在方法体内部，`names`被当作一个数组处理，使用了 `for`循环遍历输出每个名称。调用方法时可以直接传递多个参数，也可以传递一个数组（通过 `: _*`转换为可变参数）作为参数。
     >

### Spark的概念、特点、优势（与Hadoop对比）；Spark的集群管理框架、部署模式；

> Spark是一种快速、通用、可扩展的分布式计算系统，具有以下概念、特点和优势：

1. 概念：
   1. Resilient Distributed Datasets (RDDs)：Spark的核心数据抽象，是不可变的分布式对象集合，可以在集群上并行操作。
   2. 数据流（Dataflow）：Spark通过数据流的方式进行计算，可以通过多个转换操作构建数据处理流水线。
   3. 转换操作（Transformations）和动作操作（Actions）：Spark提供了丰富的转换和动作操作，可以对RDD进行转换和操作。
   4. 延迟计算（Lazy Evaluation）：Spark采用延迟计算的方式，只有在遇到动作操作时才会触发计算。
   5. 内存计算（In-Memory Computing）：Spark将数据存储在内存中，以提高计算性能。
2. 特点：
   1. 快速性能：Spark具有高速的内存计算能力和优化的执行引擎，可以加速数据处理任务的执行。
   2. 简单易用：Spark提供了简洁的API，支持多种编程语言（如Scala、Java、Python）进行开发，并提供了丰富的库和工具。
   3. 可扩展性：Spark可以轻松地扩展到大规模的数据集和集群，利用分布式计算能力进行并行处理。
   4. 多种数据处理模式：Spark支持批处理、交互式查询和流式处理等多种数据处理模式，适用于不同的应用场景。
3. 优势（与Hadoop对比）：
   1. 更快的计算速度：相比于基于磁盘的计算框架（如Hadoop MapReduce），Spark利用内存计算和数据缓存等技术，可以显著提高计算速度。
   2. 更灵活的数据处理：Spark提供了丰富的转换和动作操作，可以进行复杂的数据处理和分析，支持SQL查询、图计算、机器学习等多种数据处理任务。
   3. 更强大的内存管理：Spark能够更有效地管理内存资源，包括内存数据存储、数据分片、数据共享等，提供更高效的内存计算能力。
   4. 更广泛的生态系统：Spark具有庞大的生态系统，包括Spark SQL、Spark Streaming、MLlib（机器学习库）和GraphX（图计算库）等，支持多种数据处理和分析需求。
4. Spark的集群管理框架和部署模式：
   1. Spark可以在多种集群管理框架上运行，包括Standalone、Hadoop YARN、Apache Mesos等。
   2. 在Standalone模式下，Spark自带了一个简单的集群管理器，可以方便地在独立的Spark集群上部署和运行。
   3. 在Hadoop YARN模式下，Spark可以利用YARN作为资源管理器，与其他Hadoop生态系统工具（如HDFS）无缝集成。
   4. 在Mesos模式下，Spark可以与Mesos集成，共享Mesos集群资源进行分布式计算。

### RDD的特点及五大特性，RDD算子的分类及区别，常用的操作类算子和行动类算子；

1. RDD（Resilient Distributed Datasets）是Spark的核心数据抽象，它具有以下特点：
   * 弹性：RDD具有容错性，可以在节点故障时自动恢复数据，保证计算的可靠性。
   * 分区：RDD将数据分割为多个分区，每个分区可以在不同的节点上并行处理，实现数据的并行计算。
   * 可变性：RDD支持两种类型的操作，即转换操作和行动操作。转换操作生成新的RDD，而行动操作将RDD的结果返回给驱动程序或写入外部存储。
   * 惰性计算：RDD采用惰性计算方式，只有在遇到行动操作时才会触发计算，可以优化计算过程。
   * 缓存：RDD支持将数据缓存在内存中，以便在后续的计算中复用，提高计算性能。
2. RDD算子可分为两类：转换操作和行动操作。

### Spark的核心模块及每个模块的功能；

1. Spark Core：
   1. Spark Core是Spark的基础模块，提供了Spark的基本功能和API。
   2. 包括任务调度、内存管理、错误恢复、分布式存储等核心功能，是其他模块的基础。
2. Spark SQL：
   1. Spark SQL是Spark用于处理结构化数据的模块，提供了用于查询结构化数据的API和SQL查询语言。
   2. 它支持将结构化数据（如JSON、Parquet、Hive表等）加载到RDD中，并提供了DataFrame和DataSet这两个高级抽象，使得可以在结构化数据上进行高性能的数据处理和分析。
3. Spark Streaming：
   1. Spark Streaming是Spark用于处理实时流数据的模块，它支持将实时数据流划分为小的批次进行处理。
   2. 它提供了类似于Spark Core的API，可以在流数据上执行转换操作和行动操作，实现实时数据的处理和分析。
4. MLlib：
   1. MLlib是Spark的机器学习库，提供了丰富的机器学习算法和工具，使得在大规模数据上进行机器学习变得更加容易。
   2. MLlib支持常见的机器学习任务，如分类、回归、聚类、推荐等，并提供了分布式的算法实现。
5. GraphX：
   1. GraphX是Spark的图计算库，提供了用于图计算的API和算法。
   2. 它将图表示为分布式的属性图，并提供了一组用于处理图结构的操作，如图的构建、遍历、连通性分析、图算法等。

### RDD的创建方式有几种？每种方式如何创建，可以进行代码举例说明；

在Spark中，可以使用以下几种方式创建RDD：

1. 从已存在的集合或数据源创建RDD：
   * 可以通过并行化已有的集合或读取外部数据源来创建RDD。
     ```scala
     // 从集合创建RDD
     val rdd1 = sparkContext.parallelize(Seq(1, 2, 3, 4, 5))
     // 从文件读取创建RDD
     val rdd2 = sparkContext.textFile("path/to/file.txt")
     ```
2. 通过转换已有的RDD创建新的RDD：
   * 可以对现有的RDD应用转换操作来创建新的RDD。
     ```scala
     val rdd1 = sparkContext.parallelize(Seq(1, 2, 3, 4, 5))
     // 使用map转换操作创建新的RDD
     val rdd2 = rdd1.map(_ * 2)
     // 使用filter转换操作创建新的RDD
     val rdd3 = rdd1.filter(_ > 3)
     ```
3. 通过外部存储系统（如HDFS）创建RDD：
   * 可以通过读取外部存储系统中的数据来创建RDD。
     ```scala
     val rdd = sparkContext.textFile("hdfs://path/to/file.txt")
     ```
4. 通过并行化已有的集合创建PairRDD（键值对RDD）：
   * 可以将已有的集合转换为键值对形式的RDD。
     ```scala
     val pairs = Seq(("a", 1), ("b", 2), ("c", 3))
     val pairRDD = sparkContext.parallelize(pairs)
     ```
5. 通过外部数据源创建PairRDD（键值对RDD）：
   1. 可以从外部数据源中读取数据，并将其转换为键值对形式的RDD。
      ```scala
      val pairRDD = sparkContext.textFile("path/to/file.txt").map(line => {
      	val Array(key, value) = line.split(",")
      	(key, value.toInt)
      })
      ```

### 宽依赖于窄依赖的区别，Stage划分的依据；

1. 区别
   * 窄依赖
     * 指每个父RDD的一个Partition最多被子RDD的一个Partition所使用
     * 父RDD的一个分区对应子RDD的一个分区（一对一的关系）
     * 父RDD的多个分区对应了子RDD的一个分区（多对一的关系）
   * 宽依赖
     * 指一个父RDD的一个Partition会被子RDD的多个Partition所使用
     * 父RDD的一个分区对应了子RDD的多个分区（一对多）
     * 父RDD的多个分区对应子RDD的多个分区（多对多）
2. Stage划分
   1. 当存在宽依赖时，Spark将生成一个新的阶段，将具有宽依赖的转换操作划分到同一个阶段中。
   2. 通过将转换操作划分为多个阶段，Spark可以实现更好的并行化和任务调度，提高整体作业的性能和效率。
   3. RDD是一个大的数据集合，该集合被划分成多个子集合分布到了不同的节点上，而每个子集合就称为分区（Partition）。

### RDD缓存的方法，缓存的存储级别有哪些？默认的是哪个？

在Spark中，可以使用 `persist()`方法或 `cache()`方法对RDD进行缓存。

1. `persist()`方法提供了更多的选项来指定缓存的存储级别
2. `cache()`方法使用默认的存储级别进行缓存

存储级别（Storage Level）是指定RDD缓存的持久化方式和内存使用策略。Spark提供了以下存储级别：

1. `MEMORY_ONLY`：将RDD以反序列化的Java对象的形式存储在JVM堆内存中。如果内存空间不足，部分分区的数据可能无法缓存，并需要在需要时重新计算。
2. `MEMORY_AND_DISK`：将RDD以反序列化的Java对象的形式存储在内存中。如果内存空间不足，多余的分区将被存储在磁盘上。数据读取时会先从内存中读取，如果缓存的分区在内存中不存在，则从磁盘加载。
3. `MEMORY_ONLY_SER`：将RDD以序列化的形式存储在JVM堆内存中。相比于 `MEMORY_ONLY`，序列化可以减少内存使用，但需要进行序列化和反序列化操作。
4. `MEMORY_AND_DISK_SER`：将RDD以序列化的形式存储在内存中。如果内存空间不足，多余的分区将被存储在磁盘上。数据读取时会先从内存中读取，如果缓存的分区在内存中不存在，则从磁盘加载。
5. `DISK_ONLY`：将RDD以反序列化的Java对象的形式存储在磁盘上。
6. `OFF_HEAP`：将RDD以序列化的形式存储在堆外内存中，可以减轻JVM堆内存的压力。

默认情况下，`cache()`方法使用 `MEMORY_ONLY`存储级别进行缓存。如果需要使用其他存储级别，可以使用 `persist()`方法并传递对应的存储级别参数。

示例代码：

```scala
import org.apache.spark.storage.StorageLevel

val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))
rdd.persist(StorageLevel.MEMORY_AND_DISK)
```

以上代码将RDD使用 `MEMORY_AND_DISK`存储级别进行缓存。

### 累加器的概念、特点

* 累加器的概念：
  * 累加器是一种只能通过关联操作进行`add`操作的变量。
* 特点：
  * 能够高效的应用于并行操作中。
  * 能够用来实现counters和sums。
  * 只有在行动操作中才会触发累加器。
  * 使用 `accumulator`时，为了保证准确性，只使用一次 `action`操作。

在Spark中，可以通过 `SparkContext`的 `accumulator()`方法创建累加器，并使用 `add()`方法将值累加到累加器中。最后，可以使用 `value`属性获取累加器的最终值。

示例代码：

```scala
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("AccumulatorExample").setMaster("local[*]")
val sc = new SparkContext(conf)

val accumulator = sc.longAccumulator("sumAccumulator")

val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))
rdd.foreach(x => accumulator.add(x))

println("Accumulator value: " + accumulator.value)

sc.stop()
```

以上代码创建了一个名为 `sumAccumulator`的长整型累加器，并将RDD中的元素累加到累加器中。最后，打印出累加器的最终值。

### Spark SQL的概念；SparkSQL编程的入口；Spark RDD编程的入口；SparkSQL的两大常用数据集合；

Spark SQL是Apache Spark生态系统中的一个组件，它提供了用于处理结构化数据的高级数据处理和查询功能。Spark SQL支持使用SQL查询语言、DataFrame API和DataSet API进行数据操作和分析。

编程入口：

1. SparkSQL编程的入口是 `SparkSession`。通过创建 `SparkSession`对象，可以在Spark应用程序中使用Spark SQL功能。`SparkSession`是与Spark SQL交互的主要入口点，它负责创建和管理DataFrame和DataSet，并提供了执行SQL查询的方法。

示例代码：

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("SparkSQLExample")
  .master("local[*]")
  .getOrCreate()

// 在这里可以使用 Spark SQL 的功能进行数据处理和查询
spark.stop()
```

2. Spark RDD编程的入口是 `SparkContext`。`SparkContext`是Spark的主要入口点，用于创建和管理RDD（弹性分布式数据集）。在RDD编程中，可以使用 `SparkContext`对象执行各种操作，如创建RDD、应用转换操作和触发行动操作。

示例代码：

```scala
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("SparkRDDExample").setMaster("local[*]")
val sc = new SparkContext(conf)

// 在这里可以使用 Spark RDD 的功能进行数据处理和分析

sc.stop()
```

常用的数据集合：

1. DataFrame：DataFrame是一种分布式的数据集合，以命名列的形式组织，并且具有类似于关系型数据库中表的结构。DataFrame可以使用Spark SQL的SQL语法进行查询和操作，还可以通过DataFrame API进行数据处理。
2. DataSet：DataSet是Spark 1.6版本引入的一种数据集合类型，它是DataFrame的扩展，提供了类型安全的数据操作和查询功能。DataSet结合了RDD的强类型特性和DataFrame的高级查询功能，既可以使用强类型的对象进行操作，又可以通过SQL查询和DataFrame API进行数据处理。

这两种数据集合都提供了高效的数据处理和查询功能，并且可以无缝地集成Spark SQL的功能。可以根据具体需求选择使用DataFrame还是DataSet来进行数据操作和分析。

### SparkRDD应用程序（读取本地或分布式文件系统中的文件，对文件内容进行特殊处理，并完成统计并输出到本地文件系统或分布式文件系统中）

下面是一个使用Spark RDD编程读取本地文件系统中的文件，对文件内容进行特殊处理，并完成统计并输出到本地文件系统的示例代码：

```scala
import org.apache.spark.{SparkConf, SparkContext}

object SparkRDDExample {
  def main(args: Array[String]): Unit = {
    // 创建SparkConf对象
    val conf = new SparkConf().setAppName("SparkRDDExample").setMaster("local[*]")
  
    // 创建SparkContext对象
    val sc = new SparkContext(conf)
  
    // 读取文件内容，创建RDD
    val linesRDD = sc.textFile("file:///path/to/input/file.txt")
  
    // 对每一行进行特殊处理（示例：将每行的单词转换为大写）
    val processedRDD = linesRDD.map(line => line.toUpperCase())
  
    // 统计单词数量
    val wordCountRDD = processedRDD
      .flatMap(line => line.split("\\s+")) // 按空格分割单词
      .map(word => (word, 1)) // 将每个单词映射为(key, value)对，value为1
      .reduceByKey(_ + _) // 根据key进行求和
  
    // 将统计结果输出到本地文件系统
    wordCountRDD.saveAsTextFile("file:///path/to/output/directory")
  
    // 停止SparkContext对象
    sc.stop()
  }
}
```

在上述代码中，需要替换以下部分以适应你的实际情况：

- `file:///path/to/input/file.txt`：输入文件的路径，可以是本地文件系统或分布式文件系统中的文件路径。
- `file:///path/to/output/directory`：输出结果的目录路径，可以是本地文件系统或分布式文件系统中的目录路径。

该示例代码使用 `textFile`方法读取文件内容并创建RDD，使用 `map`和 `flatMap`等转换操作对RDD进行处理，使用 `reduceByKey`进行聚合操作，最后使用 `saveAsTextFile`将结果保存到指定的输出目录中。

请确保在运行代码之前已正确配置Spark环境，并将示例代码中的文件路径替换为实际的文件路径。

## SparkRDD算子

转换算子（Transformation Operators）：

1. map(func)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.map(x => x * 2)
result.collect()  // 输出: Array(2, 4, 6, 8, 10)
```

2. flatMap(func)

```scala
val rdd = sc.parallelize(List("Hello World", "Spark is awesome"))
val result = rdd.flatMap(x => x.split(" "))
result.collect()  // 输出: Array("Hello", "World", "Spark", "is", "awesome")
```

3. mapPartitions(func)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5), 2)  // 2个分区
val result = rdd.mapPartitions(iter => Iterator(iter.sum))
result.collect()  // 输出: Array(3, 12)
```

4. mapPartitionsWithIndex(func)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5), 2)  // 2个分区
val result = rdd.mapPartitionsWithIndex((index, iter) => Iterator((index, iter.sum)))
result.collect()  // 输出: Array((0, 3), (1, 12))
```

5. reduceByKey(func)

```scala
val rdd = sc.parallelize(List(("a", 1), ("b", 2), ("a", 3), ("b", 4)))
val result = rdd.reduceByKey((x, y) => x + y)
result.collect()  // 输出: Array(("a", 4), ("b", 6))
```

6. sortBy(func, ascending)

```scala
val rdd = sc.parallelize(List(5, 3, 1, 4, 2))
val result = rdd.sortBy(x => x, ascending = true)
result.collect()  // 输出: Array(1, 2, 3, 4, 5)
```

7. sortByKey(ascending)

```scala
val rdd = sc.parallelize(List(("b", 2), ("a", 1), ("c", 3)))
val result = rdd.sortByKey(ascending = true)
result.collect()  // 输出: Array(("a", 1), ("b", 2), ("c", 3))
```

8. filter(func)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.filter(x => x % 2 == 0)
result.collect()  // 输出: Array(2, 4)
```

9. distinct()

```scala
val rdd = sc.parallelize(List(1, 2, 3, 2, 1))
val result = rdd.distinct()
result.collect()  // 输出: Array(1, 2, 3)
```

10. union(rdd)

```scala
val rdd1 = sc.parallelize(List(1, 2, 3))
val rdd2 = sc.parallelize(List(4, 5, 6))
val result = rdd1.union(rdd2)
result.collect()  // 输出: Array(1, 2, 3, 4, 5, 6)
```

11. intersection(rdd)

```scala
val rdd1

 = sc.parallelize(List(1, 2, 3, 4, 5))
val rdd2 = sc.parallelize(List(4, 5, 6))
val result = rdd1.intersection(rdd2)
result.collect()  // 输出: Array(4, 5)
```

12. subtract(rdd)

```scala
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5))
val rdd2 = sc.parallelize(List(4, 5, 6))
val result = rdd1.subtract(rdd2)
result.collect()  // 输出: Array(1, 2, 3)
```

13. groupByKey()

```scala
val rdd = sc.parallelize(List(("a", 1), ("b", 2), ("a", 3), ("b", 4)))
val result = rdd.groupByKey()
result.collect()  // 输出: Array(("a", Iterable(1, 3)), ("b", Iterable(2, 4)))
```

行动算子（Action Operators）：

1. reduce()

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.reduce((x, y) => x + y)
println(result)  // 输出: 15
```

2. collect()

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.collect()
println(result.mkString(", "))  // 输出: 1, 2, 3, 4, 5
```

3. count()

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.count()
println(result)  // 输出: 5
```

4. first()

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.first()
println(result)  // 输出: 1
```

5. take(num)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val result = rdd.take(3)
println(result.mkString(", "))  // 输出: 1, 2, 3
```

6. countByKey()

```scala
val rdd = sc.parallelize(List(("a", 1), ("b", 2), ("a", 3), ("b", 4)))
val result = rdd.countByKey()
println(result)  // 输出: Map("a" -> 2, "b" -> 2)
```

7. saveAsTextFile(path)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
rdd.saveAsTextFile("output/path")
```

8. foreach(func)

```scala
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
rdd.foreach(x => println(x))
```
